{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d69867",
   "metadata": {},
   "source": [
    "# Retrieving search results from the UniProt API with Python\n",
    "\n",
    "This document describes methods of downloading search results using the UniProt API. All examples here use `Python 3` with the `requests` library. There are repeated imports to faciliate quick copy and paste into your code.\n",
    "\n",
    "## 1. Use search results immediately within a Python script\n",
    "\n",
    "### 1.1 Small number of results: use stream\n",
    "\n",
    "In this context we want to fetch a small number of search results and save these into a variable in memory so these can then be utilised within the script. To motivate this we will download the reviewed entries from the organism SARS-CoV-2 in UniProtKB:\n",
    "\n",
    "#### Stream limitations\n",
    "\n",
    "* The stream endpoint is expensive for the API to process and for this reason there is a limitation on the number of parallel requests it will handle. In the case of the stream API having too many requests a `429` status will be returned in which case you can either use pagination (see 1.2) or try stream again later.\n",
    "* The stream endpoint can handle at most result sets with 5,000,000 entries. If you require more consider: looking at FTP downloads; reduce your search by being more specific; using pagination (see 1.2)\n",
    "\n",
    "#### Steps\n",
    "1. Visit https://www.uniprot.org/uniprotkb?query=(organism_id:2697049)%20AND%20(reviewed:true)\n",
    "2. Click `Download` in the toolbar\n",
    "3. For this example we want the following selected:\n",
    " - Download all\n",
    " - Format: FASTA (canonical)\n",
    " - Compressed: No\n",
    "4. Click `Generate URL for API`\n",
    "5. Under the header `API URL using the streaming endpoint` click `Copy` to get the URL which will be used in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f8ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://rest.uniprot.org/uniprotkb/stream?compressed=false&format=fasta&query=%28organism_id%3A2697049%29%20AND%20%28reviewed%3Atrue%29'\n",
    "all_fastas = requests.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3424652",
   "metadata": {},
   "source": [
    "At this point the `all_fastas` will contain a single string of all the FASTA sequences. From here we can create a list of FASTA sequences and find all sequences with header mentioning `SPIKE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a10f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>sp|P0DTC2|SPIKE_SARS2 Spike glycoprotein OS=Severe acute respiratory syndrome coronavirus 2 OX=2697049 GN=S PE=1 SV=1\\nMFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFS\\nNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIV\\nNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLE\\nGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQT\\nLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETK\\nCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISN\\nCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIAD\\nYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPC\\nNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVN\\nFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITP\\nGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSY\\nECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTI\\nSVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQE\\nVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDC\\nLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAM\\nQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALN\\nTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRA\\nSANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPA\\nICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDP\\nLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDL\\nQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDD\\nSEPVLKGVKLHYT']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "fasta_list = re.split(r'\\n(?=>)', all_fastas)\n",
    "[fasta for fasta in fasta_list if 'SPIKE' in fasta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81232a5",
   "metadata": {},
   "source": [
    "### 1.2 Large number of results: use pagination\n",
    "\n",
    "When there are a large number of results to fetch it is advisable to use \"pagination\" which means fetching batches of results one at a time. This is preferable to streaming because:\n",
    "\n",
    "1. Small memory footprint: if the search result data exceeds your computer's memory, downloading by streaming this at once will cause your Python script to crash. Pagiation will only load a subset of the results into your memory at once.\n",
    "2. Robust to connection issues: if when downloading by streaming the connection is interrupted, the download will need to be completely restarted. When working with pagiation, each batch can be reattempted from the point of failure without requiring to restart.\n",
    "3. Less API resource demand: the stream endpoint is very expensive as it requires a large amount of memory. The pagination endpoint distributes this resource demand over a longer period of time so the API infrastructure is better to deal with this.\n",
    "4. Process each batch as it arrives: downloading by streaming requires the download to be complete before any processing can take place. Batching allows processing to be interleaved with downloading which may be useful should it be desired to see processed results as soon as possible. The following diagram illustrates this:\n",
    "\n",
    "```\n",
    "Stream             Pagination\n",
    "+------------+     +------------+ \n",
    "| Download n |     | Download n |\n",
    "| Download n |     | Process  n |\n",
    "| Download n |     | Download n |\n",
    "| Process  n |     | Process  n |\n",
    "| Process  n |     | Download n |\n",
    "| Process  n |     | Process  n |\n",
    "+------------+     +------------+\n",
    "```\n",
    "\n",
    "To motivate this context we want to find reviewed UniProtKB entries containing the word `Insulin` and sorted by the greatest number of interactions.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. View the results in the browser here: https://www.uniprot.org/uniprotkb?query=Insulin%20AND%20(reviewed:true)\n",
    "2. Click `Download` in the toolbar\n",
    "3. For this example we want the following selected:\n",
    " - Download all\n",
    " - Format: TSV\n",
    " - Compressed: No\n",
    "4. Click `Generate URL for API`\n",
    "5. Under the header `API URL using the search endpoint` click `Copy` to get the URL which will be used in the following snippet.\n",
    "6. Always use `size=500` as this will provide fast performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cf1e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 4930\n",
      "1000 / 4930\n",
      "1500 / 4930\n",
      "2000 / 4930\n",
      "2500 / 4930\n",
      "3000 / 4930\n",
      "3500 / 4930\n",
      "4000 / 4930\n",
      "4500 / 4930\n",
      "4930 / 4930\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def get_next_link(headers):\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "def get_batch(batch_url):\n",
    "    while batch_url:\n",
    "        response = session.get(batch_url)\n",
    "        response.raise_for_status()\n",
    "        total = response.headers[\"x-total-results\"]\n",
    "        yield response, total\n",
    "        batch_url = get_next_link(response.headers)\n",
    "        \n",
    "url = 'https://rest.uniprot.org/uniprotkb/search?fields=accession%2Ccc_interaction&format=tsv&query=Insulin%20AND%20%28reviewed%3Atrue%29&size=500'\n",
    "interactions = {}\n",
    "for batch, total in get_batch(url):\n",
    "    for line in batch.text.splitlines()[1:]:\n",
    "        primaryAccession, interactsWith = line.split('\\t')\n",
    "        interactions[primaryAccession] = len(interactsWith.split(';')) if interactsWith else 0\n",
    "    print(f'{len(interactions)} / {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca051ecb",
   "metadata": {},
   "source": [
    "Finally to see the accessions with the greatest number of interactions the `interactions` dict is sorted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ebd5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O76024', 467),\n",
       " ('P05067', 463),\n",
       " ('Q9NRD5', 327),\n",
       " ('O14901', 231),\n",
       " ('P62993', 197),\n",
       " ('O60260', 171),\n",
       " ('O43741', 156),\n",
       " ('P46379', 150),\n",
       " ('P61981', 144),\n",
       " ('P01112', 128)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_interactions = sorted(interactions.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_interactions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23eda27",
   "metadata": {},
   "source": [
    "## 2. Save search results to disk\n",
    "\n",
    "### 2.1 Small number of results: use stream\n",
    "\n",
    "In this context we want to save to disk search results which have a small number of entries. The example from 1.1 will be used.\n",
    "\n",
    "#### Stream limitations\n",
    "\n",
    "* The stream endpoint is expensive for the API to process and for this reason there is a limitation on the number of parallel requests it will handle. In the case of the stream API having too many requests a `429` status will be returned in which case you can either use pagination (see 2.2) or try stream again later.\n",
    "* The stream endpoint can handle at most result sets with 5,000,000 entries. If you require more consider: looking at FTP downloads; reduce your search by being more specific; using pagination (see 2.2)\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Visit https://www.uniprot.org/uniprotkb?query=(organism_id:2697049)%20AND%20(reviewed:true)\n",
    "2. Click `Download` in the toolbar\n",
    "3. For this example we want the following selected:\n",
    " - Download all\n",
    " - Format: FASTA (canonical)\n",
    " - Compressed: Yes\n",
    "4. Click `Generate URL for API`\n",
    "5. Under the header `API URL using the streaming endpoint` click `Copy` to get the URL which will be used in the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105ff03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://rest.uniprot.org/uniprotkb/stream?compressed=true&format=fasta&query=%28organism_id%3A2697049%29%20AND%20%28reviewed%3Atrue%29'\n",
    "with requests.get(url, stream=True) as request:\n",
    "    request.raise_for_status()\n",
    "    with open('SARS-CoV-2.fasta.gz', 'wb') as f:\n",
    "        for chunk in request.iter_content(chunk_size=2**20):\n",
    "            f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bb19be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">sp|P0DTC1|R1A_SARS2 Replicase polyprotein 1a OS=Severe acute respiratory syndrome coronavirus 2 OX=2697049 PE=1 SV=1\r\n",
      "MESLVPGFNEKTHVQLSLPVLQVRDVLVRGFGDSVEEVLSEARQHLKDGTCGLVEVEKGV\r\n",
      "LPQLEQPYVFIKRSDARTAPHGHVMVELVAELEGIQYGRSGETLGVLVPHVGEIPVAYRK\r\n",
      "VLLRKNGNKGAGGHSYGADLKSFDLGDELGTDPYEDFQENWNTKHSSGVTRELMRELNGG\r\n",
      "AYTRYVDNNFCGPDGYPLECIKDLLARAGKASCTLSEQLDFIDTKRGVYCCREHEHEIAW\r\n",
      "YTERSEKSYELQTPFEIKLAKKFDTFNGECPNFVFPLNSIIKTIQPRVEKKKLDGFMGRI\r\n",
      "RSVYPVASPNECNQMCLSTLMKCDHCGETSWQTGDFVKATCEFCGTENLTKEGATTCGYL\r\n",
      "PQNAVVKIYCPACHNSEVGPEHSLAEYHNESGLKTILRKGGRTIAFGGCVFSYVGCHNKC\r\n",
      "AYWVPRASANIGCNHTGVVGEGSEGLNDNLLEILQKEKVNINIVGDFKLNEEIAIILASF\r\n",
      "SASTSAFVETVKGLDYKAFKQIVESCGNFKVTKGKAKKGAWNIGEQKSILSPLYAFASEA\r\n"
     ]
    }
   ],
   "source": [
    "!gzcat SARS-CoV-2.fasta.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e09b16",
   "metadata": {},
   "source": [
    "### 2.2 Large number of results: use pagination\n",
    "\n",
    "When dealing with large numbers of results, as in section 1.2, it is preferable to use the pagination endpoint. Points 2 and 3 from section 1.2 still apply:\n",
    "\n",
    "2. Robust to connection issues\n",
    "3. Less API resource demand\n",
    "\n",
    "The example from 1.2 will be used.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. View the results in the browser here: https://www.uniprot.org/uniprotkb?query=Insulin%20AND%20(reviewed:true)\n",
    "2. Click `Download` in the toolbar\n",
    "3. For this example we want the following selected:\n",
    " - Download all\n",
    " - Format: TSV\n",
    " - Compressed: No\n",
    "4. Click `Generate URL for API`\n",
    "5. Under the header `API URL using the search endpoint` click `Copy` to get the URL which will be used in the following snippet.\n",
    "6. Always use `size=500` as this will provide fast performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b79f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 / 4930\n",
      "1000 / 4930\n",
      "1500 / 4930\n",
      "2000 / 4930\n",
      "2500 / 4930\n",
      "3000 / 4930\n",
      "3500 / 4930\n",
      "4000 / 4930\n",
      "4500 / 4930\n",
      "4930 / 4930\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def get_next_link(headers):\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "def get_batch(batch_url):\n",
    "    while batch_url:\n",
    "        response = session.get(batch_url)\n",
    "        response.raise_for_status()\n",
    "        total = response.headers[\"x-total-results\"]\n",
    "        yield response, total\n",
    "        batch_url = get_next_link(response.headers)\n",
    "        \n",
    "\n",
    "url = 'https://rest.uniprot.org/uniprotkb/search?fields=accession%2Ccc_interaction&format=tsv&query=Insulin%20AND%20%28reviewed%3Atrue%29&size=500'\n",
    "progress = 0\n",
    "with open('insulin-interactions.tsv', 'w') as f:\n",
    "    for batch, total in get_batch(url):\n",
    "        lines = batch.text.splitlines()\n",
    "        if not progress:\n",
    "            print(lines[0], file=f)\n",
    "        for line in lines[1:]:\n",
    "            print(line, file=f)\n",
    "        progress += len(lines[1:])\n",
    "        print(f'{progress} / {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2196b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry\tInteracts with\r\n",
      "P06213\tQ99490; Q8NEJ0; Q13322; P05019; P35568; Q15323; P27986; P19174; P18031; Q06124; Q15256; Q9NRF2; P29353; P01317; P35570; P32121; P06213-1; P18031; Q92485-2; P81122; Q1XH17; P01308; Q13257; Q92485-2; P01317\r\n",
      "P14735\tP05067; P10147; PRO_0000000093 [P05067]; P01275; P10997; P14735-1; P01308; Q9J3M8\r\n",
      "P35222\tP01023; P00519; O43707; Q5JTC6; P25054; P10275; O15169; Q9Y2T1; O00512; A1Z199; P23560-2; Q9Y297; P55212; P35520; Q6P1J9; P12830; P19022; P33151; Q92793; P02511; P35221; Q9UI47-1; Q9NSA3; Q9NYF0; G5E9A7; P26358; P18146; P50402; P29317; Q9UKB1; Q96AC1; Q92915-2; P49789; Q08050; P04406; P14136; Q9BZE0; P49841; Q9UBN7; Q16665; O00291; P07900; P42858; P08069; P46940; Q8WXH2; O75564; Q14678; Q2LD37; P13473-2; Q9UJU2; Q8WVC0; Q14114-3; Q9GZQ8; P51608; P55197; Q92597; P19404; P19838; P29474; O00482-1; P49757; P49757-3; P14618; P14618-1; O75400-2; Q03431; Q13308; P08575; P23470; Q12913; P49023; P62826; Q04206; Q13761; Q9Y265; Q01826; Q9H6I2; Q7Z699; P12931; P15884; P36402; Q9NQB0; O14746; Q9UKE5; P11388; O14656-2; Q13625; O95071; Q9GZV5; P46937; P46937-3; O35625; Q9YGY0; Q67FY2; P33724; P26231; P18012; P27782; Q01887; Q9DBG9; Q9EPK5\r\n",
      "P51451\tP10275; Q8NDB2; Q13490; P46109; O43281; O43281-2; P00533; Q96Q35-2; P51114-2; P51116; Q13480; Q13322-4; O75031; P08238; Q9UKT9; Q96N16; P10721; Q9UJV3-2; Q9NRD5; P27986-2; Q92569; P49023-2; O14796; Q13239-3; O14543; Q9BWW4; Q9UGK3; P40763; O95551; P08670; A0A0C4DGF1; B2RXF5; Q96BR9; Q6NX45\r\n",
      "P04004\tQ07021; Q15700; Q92796; Q9HD26; Q9HD26-2; Q9NSN8; Q9UHD9; P75358; P75167; P78007; A0A024A2C9; P75390; P75391; P78031; P75611; Q4KTX9\r\n",
      "P49862\t\r\n",
      "Q15119\tP42858\r\n",
      "Q86YN6\t\r\n",
      "Q9BX66\tP00519; O15265; P42858; O15357; Q13177; Q6NSK7; P39052; A0A061I5T4; Q8WWV3\r\n"
     ]
    }
   ],
   "source": [
    "!head insulin-interactions.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
